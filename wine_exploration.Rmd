---
title: "wine_exploration"
author: "Chris DiSerafino"
date: "4/14/2020"
output: pdf_document
---

```{r setup, include=FALSE}
library(dplyr)
library(neuralnet)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
wines <- read.csv("wines.csv")
quality_wines <- read.csv("wine_quality.csv")
color_wines <- read.csv("wine_color.csv")
```

## R Markdown

Research question: how well can we classify a wine as red or white using neural networks? Does the number of hidden units or layers matter? How does starting values or scaling effect our results? The risk function of neural networks is not convex. Which method of finding a global minima is most effective in this instance?

## Explore Data

```{r, include =TRUE}
set.seed(13)
head(quality_wines)
head(color_wines)

#look at distribution of wine quality
ggplot(wines) + 
  geom_histogram(aes(x = quality), binwidth = 1) +
  ggtitle("Histogram of wine quality")

#how would we want to classify these wines?


#Run a PCA
pcs = prcomp(color_predict)

#Summarize the pcs
summary(pcs)
screeplot(pcs, type = "lines", main = "Variance explained by PC")

plot(pcs$x[,1], 
      main = paste("PC", eval(1)), 
      xlab = "Sample Index",
      ylab = paste("PC", eval(1), "score"), 
      col = color_response + 1)

#first pc seems to do most of work in seperating by wine color

plot(pcs$x[,1:2], 
     main = "Biplot of Wine Data by Color", 
     xlab = "PC 1", ylab = "PC 2",
     col = color_response + 1)
barplot(table(color_response), col = unique(color_response + 1), legend.text = c("white", "red"))

plot(pcs$x[,1], 
      main = paste("PC", eval(1)), 
      xlab = "Sample Index",
      ylab = paste("PC", eval(1), "score"), 
      col = quality_response)

plot(pcs$x[,1:2], 
     main = "Biplot of Wine Data by Quality", 
     xlab = "PC 1", ylab = "PC 2",
     col = quality_response)
barplot(table(quality_response), col = unique(quality_response))

#only show first two pcs because they account for most of variance
k.pca = kmeans(pcs$x[,1:2], centers = 2)
cluster_update = as.factor(k.pca$cluster)
x <- pcs$x
ggplot() +
  geom_point(aes(x = x[,1], y = x[,2], col = cluster_update))+
  xlab("First PC") + 
  ylab("Second PC") +
  scale_colour_discrete(name = "Cluster") +
  ggtitle("First Two PCs of Gene Data; Colored by Cluster of first 2 PCs")

#clustering using kmeans with 2 centers seems to be a good approximation of color
```

Analysis: One PC explains over 95 percent of the variance, and two PCs explain over 99 percent of the variance! This is something we should definitely mention, we can plot to two dimensions and still have most of the variance accounted for

## Create Neural Network

```{r}
max = apply(wines, 2 , max)
min = apply(wines, 2 , min)
wines = as.data.frame(scale(wines, center = min, scale = max - min))

training_size = round(.75 * nrow(wines))
indices = sample(1:nrow(wines), training_size)
training_set = wines[indices,]
testing_set = wines[-(indices),]


NN = neuralnet(type ~ ., training_set, hidden = 3 , linear.output = F )

# plot neural network
plot(NN)

```


```{r}
predict_testNN = compute(NN, testing_set[,c(1:12)])
predict_testNN = predict_testNN$net.result

predicted_labels = c();
predicted_labels = (predict_testNN[,1] >= 0.5) *1

# Calculate Risk
nn_risk = sum(testing_set$type == predicted_labels)/length(predicted_labels)
if (nn_risk >= 0.5) {
  nn_risk = 1 - nn_risk
}
nn_risk_test = nn_risk

predict_testNN = compute(NN, training_set[,c(1:12)])
predict_testNN = predict_testNN$net.result

predicted_labels = c();
predicted_labels = (predict_testNN[,1] >= 0.5) *1

# Calculate Risk
nn_risk = sum(training_set$type == predicted_labels)/length(predicted_labels)
if (nn_risk >= 0.5) {
  nn_risk = 1 - nn_risk
}
nn_risk_train = nn_risk

model = c(1, 1)
error = c(nn_risk_train, nn_risk_test)
data = c("train", "test")
total_error = data.frame(cbind(model, error, data))

ggplot(total_error) + 
  geom_boxplot(aes( x = model, y = error, color = data)) +
  ggtitle("Testing and Training Error Rate of Models")
```

