---
title: "wine_exploration"
author: "Chris DiSerafino"
date: "4/14/2020"
output: pdf_document
---

```{r setup, include=FALSE}
library(dplyr)
library(neuralnet)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
reds <- read.csv("winequality-red.csv", sep = ";")
whites <- read.csv("winequality-white.csv", sep = ";")
```

## R Markdown

Research question: how well can we classify a wine as red or white using neural networks? Does the number of hidden units or layers matter? How does starting values or scaling effect our results? The risk function of neural networks is not convex. Which method of finding a global minima is most effective in this instance?

## Explore Data

```{r, include =TRUE}
set.seed(13)
head(reds) #look at red wines
head(whites) #look at white wines

whites[,13] = -1
reds[,13] = 1
wines = rbind(whites, reds)
#data set to classify wines by quality
quality_wines = wines 
#data set to classify wines by color
wines = wines %>% rename("type" = "V13")
color_wines = wines[,c(1:11,13)]
color_predict = color_wines[,1:11]
color_response = color_wines[,12]
head(quality_wines)
head(color_wines)

#look at distribution of wine quality
ggplot(wines) + 
  geom_histogram(aes(x = quality), binwidth = 1) +
  ggtitle("Histogram of wine quality")

#how would we want to classify these wines?


#Run a PCA
pcs = prcomp(color_predict)

#Summarize the pcs
summary(pcs)


```

Analysis: One PC explains over 95 percent of the variance, and two PCs explain over 99 percent of the variance! This is something we should definitely mention, we can plot to two dimensions and still have most of the variance accounted for

## Create Neural Network

```{r}
max = apply(wines, 2 , max)
min = apply(wines, 2 , min)
wines = as.data.frame(scale(wines, center = min, scale = max - min))

training_size = round(.75 * nrow(wines))
indices = sample(1:nrow(wines), training_size)
training_set = wines[indices,]
testing_set = wines[-(indices),]


NN = neuralnet(type ~ ., training_set, hidden = 3 , linear.output = F )

# plot neural network
plot(NN)

```


```{r}
predict_testNN = compute(NN, testing_set[,c(1:12)])
predict_testNN = predict_testNN$net.result

predicted_labels = c();
predicted_labels = (predict_testNN[,1] >= 0.5) *1

# Calculate Risk
nn_risk = sum(testing_set$type == predicted_labels)/length(predicted_labels)
if (nn_risk >= 0.5) {
  nn_risk = 1 - nn_risk
}
nn_risk_test = nn_risk

predict_testNN = compute(NN, training_set[,c(1:12)])
predict_testNN = predict_testNN$net.result

predicted_labels = c();
predicted_labels = (predict_testNN[,1] >= 0.5) *1

# Calculate Risk
nn_risk = sum(training_set$type == predicted_labels)/length(predicted_labels)
if (nn_risk >= 0.5) {
  nn_risk = 1 - nn_risk
}
nn_risk_train = nn_risk

model = c(1, 1)
error = c(nn_risk_train, nn_risk_test)
data = c("train", "test")
total_error = data.frame(cbind(model, error, data))

ggplot(total_error) + 
  geom_boxplot(aes( x = model, y = error, color = data)) +
  ggtitle("Testing and Training Error Rate of Models")
```

